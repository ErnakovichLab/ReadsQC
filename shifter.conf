# This is an example of how you can use Docker only workflows as a Cromwell
# backend provider. *This is not a complete configuration file!* The
# content here should be copy pasted into the backend -> providers section
# of the cromwell.examples.conf in the root of the repository. You should
# uncomment lines that you want to define, and read carefully to customize
# the file. If you have any questions, please open an issue at
# https://www.github.com/broadinstitute/cromwell/issues

# Documentation
# This backend doesn't have an official page, but you can read about general
# Docker use here: https://cromwell.readthedocs.io/en/develop/tutorials/Containers/#docker
# If you want to use containers, the other sections on that page will be useful to you.

#backend {
#  default = "Local"

#  providers {

#    # Example backend that _only_ runs workflows that specify docker for every command.
#    Local {
#      config {
#        run-in-background = true
#        runtime-attributes = """
#        String? docker
#        String? docker_user = "$EUID" 
#        String? database
#        """
#        
#        submit = "/bin/bash ${script}"
#        #--volume ${cwd}:${docker_cwd} \
#        submit-docker = """
#		LOOKUP=$(shifterimg lookup ${docker})
#		if [[ ! $LOOKUP ]]; then
#			shifterimg pull ${docker}
#		fi
#
#		shifter --image=docker:${docker} \
#			${"--volume " + database + ":/databases"} \
#			${job_shell} ${docker_script}
#        """
#
#	dockerRoot = /global/cfs/cdirs/m3408/aim2/rqc_filter/cromwell-executions
#        filesystems {
#          local {
#            localization: [
#              "hard-link", "soft-link", "copy"
#            ]
#            caching {
#              duplication-strategy: [
#                "hard-link", "soft-link", "copy"
#              ]
#            }
#          }
#        }    
#     
#      }
#    }
#  }
#}

backend {
  default = slurm

  providers {
    slurm {
      actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"                                                                                     
      config {
        runtime-attributes = """
        Int runtime_minutes = 600
        Int cpus = 2
        Int requested_memory_mb_per_core = 8000
        String? docker
        """

        submit = """
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              -t ${runtime_minutes} \
              ${"-c " + cpus} \
              --mem-per-cpu=${requested_memory_mb_per_core} \
              --wrap "/bin/bash ${script}"
        """

        submit-docker = """
            # Make sure the SINGULARITY_CACHEDIR variable is set. If not use a default
            # based on the users home.
            if [ -z $SINGULARITY_CACHEDIR ]; 
                then CACHE_DIR=$HOME/.singularity/cache
                else CACHE_DIR=$SINGULARITY_CACHEDIR
            fi
            # Make sure cache dir exists so lock file can be created by flock
            mkdir -p $CACHE_DIR  
            LOCK_FILE=$CACHE_DIR/singularity_pull_flock
            # Create an exclusive filelock with flock. --verbose is useful for 
            # for debugging, as is the echo command. These show up in `stdout.submit`.
            flock --verbose --exclusive --timeout 900 $LOCK_FILE \
            singularity exec --containall docker://${docker} \
            echo "successfully pulled ${docker}!"

            # Submit the script to SLURM
            sbatch \
              --wait \
              -J ${job_name} \
              -D ${cwd} \
              -o ${cwd}/execution/stdout \
              -e ${cwd}/execution/stderr \
              -t ${runtime_minutes} \
              ${"-c " + cpus} \
              --mem-per-cpu=${requested_memory_mb_per_core} \
              --wrap "singularity exec --containall --bind ${cwd}:${docker_cwd} $IMAGE ${job_shell} ${docker_script}"
        """

        kill = "scancel ${job_id}"
        check-alive = "squeue -j ${job_id}"
        job-id-regex = "Submitted batch job (\\d+).*"
      }
    }
  }
}

docker {
  hash-lookup {
    enabled = false
    # Set this to match your available quota against the Google Container Engine API
    #gcr-api-queries-per-100-seconds = 1000

    # Time in minutes before an entry expires from the docker hashes cache and needs to be fetched again
    #cache-entry-ttl = "20 minutes"

    # Maximum number of elements to be kept in the cache. If the limit is reached, old elements will be removed from the cache
    #cache-size = 200

    # How should docker hashes be looked up. Possible values are "local" and "remote"
    # "local": Lookup hashes on the local docker daemon using the cli
    # "remote": Lookup hashes on docker hub and gcr
    method = "local"
  }
}
